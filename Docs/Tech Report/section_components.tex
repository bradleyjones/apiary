\section{Hive Subcomponents}

\subsection{Agentmanager and Agentmonitor}

These were the first of the Hive subcomponents to be built, the purpose of
Agentmanager and Agentmonitor are to configure and keep track of our Data Agents
(Bee).

Agents are, as described earlier in the paper, installed with no configuration.
Agentmanager’s first job is to handshake new Agents entering the environment and
provide them a unique identifier which is attached to data transmissions to the
rest of Hive. After the initial bootstrapping of an Agent, Agentmanager then
receives periodic heartbeats from the Agents. This heartbeat is used by
AgentMonitor to detect the status of an Agent in the case of an unclean
disconnection, error, network congestion, or other problem with the host
machine.

The final task performed by these two components is to provide the Agents with a
series of Data targets, in the case of the file watching Agent, this allows a
user to remotely set the files that the Agent is monitoring. The API allows this
to be done in bulk, with many targets, across many Agents, to prevent
repetitiveness. Agent status responses (success, fail) are then collated and
returned in a labeled list.

Maintaining the real time aspect of the whole application required the addition
of “events” to Agentmanager and Agentmonitor. Whenever a significant change
happens on either component; such as, a new agent handshakes into the
environment, or an agent is flagged as dead, the component where this change
happened publishes an event object onto a known message exchange, which other
services - if they are listening - can use to perform any tasks they have which
are relevant to a change in the Agent Manager without the need for polling.
Subscription to this exchange is completely open, optional and will not alter
the overall behavior of the program.

The reason for splitting this component into both Agentmanager and Agentmonitor,
is because the tasks performed by each scale differently. Agentmanager needs to
handle a constant stream of Agent heartbeats, which get more frequent, as more
Agents are added. In comparison Agent Monitors’ task does not increase in
complexity as fast, and because of this, it does not need to be scaled as
aggressively. This configuration of split services provides the greatest
flexibility for host utilisation at large scale, as you can scale very specific
parts of the whole application.

\subsection{Honeycomb}

Honeycomb is our most complicated component as this is where all the storage and
processing of collected data takes place. Honeycomb follows the standard hive
component structure, and uses a common MVC stack for inbound requests. It uses a
subscriber queue to digest the stream of data it receives as fast as possible
without having to respond to each request, which would slow the process down.

Honeycomb uses our common model structure (see Common) to verify the validity of
all received data before it is saved to the database. There is also an
additional step in the Honeycomb save function to perform indexing for every
data entry. After some research we settled on using PyLucene\cite{pylucene} as a python
wrapper for Lucene\cite{lucene} to handle indexing. Lucene is an open-source plain text
analysis and search tool developed by the Apache Software Foundation. Whenever
data is saved it is passed to Lucene for indexing so that we can utilize their
powerful query syntax, without having to apply schema to the contents of the
data.

As well as storing the data, Honeycomb also exposes an API for searching it.
These searches can be one of two types; a one-off query, or a recurring query.
One-off queries are simply a remote procedure call (RPC) containing the query.
The response is written into a JSON object and passed back to the requesting
process via the message bus. This is great for individual tasks, but in order to
facilitate real time updates we needed to introduce recurring queries. Recurring
queries require a little more setup as they need to be run concurrently
alongside the main Honeycomb thread. Since Honeycomb has to remain stateless for
scalability, it should not locally maintain any reference to these background
threads, as this would mean that a second instance of Honeycomb wouldn’t be able
to interact with the new query.

In order to solve this issue we use the message bus again, spawning a wrapper
process which provides an API to the thread in the background, and stores the
key to communicate with that wrapper process in the database. The recurring
query thread isn’t complex, simply running the lucene query then waiting a
second before running it again. The thread also maintains a list of any
previously discovered log IDs, and will only output if there has been a change,
or if it has been asked to send the next set of results. Output for these
background queries is pushed onto a uniquely named fanout exchange on the
message bus. This allows multiple processeses to receive copies of the output
from a single query, and also ensures that if no one is listening the output is
simply thrown away instead of clogging up a queue on the message bus.

\subsection{Pheromone}

Using the recurring query API in Honeycomb, Pheromone places a layer of
intelligence on top of the query to filter out specific results and trigger
alerts based on user-defined conditions. Alerters are created through the
Pheromone API. These alerters take trigger cases, and search parameters they
need to test for an alert, and a custom message that is included when an alert
is transmitted. Pheromone currently only has the one type of alerter - it will
fire an alert if a query gets a certain number of matches during a given
timescale. However it is not limited to just this one alert type, as the design
patterns used for the alerters allows for easy addition of new ones in the
future.

Pheromone uses python's multiprocessing library to start background tasks
similar to how Honeycomb’s recurring query tasks operate. When a request comes
in Pheromone starts a new alerter in the background, however unlike the
Honeycomb workers there is no unique output exchange, as alerts are always
pushed onto a known message bus routing key. This allows other services, for
example Sting, to listen for them and perform all necessary tasks they have
relative to the alert.

\subsection{Sting}

Pheromone alerts are a very powerful tool, but the user is not always going to
be sitting in front of their computer when an alert is triggered. In fact, the
very nature of user-defined alerts generally makes the times when they will
occur rare and/or unpredictable, and so we must be able to inform the user of an
alert as reliably as possible. By building a native iOS\cite{ios} app as described
later in this paper, we are able to take advantage of the Apple Push
Notification Service (APNS)\cite{apns}. APNS is a service made available by Apple to
all iOS application developers for sending small text-based messages to iOS
devices using your mobile application. In many ways this is similar to the
standard Short Message Service (SMS), but with the key differences that the
message is sent specifically to the application on the device, and more
importantly - is free. Many third-party services exist for sending automatically
triggered SMS messages (for example, Twilio\cite{twilio}), but these services charge
several pence per message, which could become very expensive in a large system
with many users and alert conditions.

When Pheromone pushes an alert onto the message bus, Sting picks it up and
looks up the user to which the alert is registered. For every iOS device
registered to that user (multiple iPhones, iPads, etc.) Sting will generate an
APNS push notification using the alert text specified by the user and the
Device ID associated with the device. The messages are then sent using the
APNS API - typically taking up to 5 seconds to arrive. This system ensures
that a user will always receive potentially critical updates about the state
of their system as and when they happen, giving them the time to act on them
before it may be too late.

\subsection{Common}

Though not technically a subcomponent in itself - Common is the base layer
underlying almost all of the hive subcomponents. Any code that was duplicated
and was reusable was moved into Common to maintain the DRY\footnote{Don’t Repeat Yourself} programming style.
Common therefore became the home for our MVC stack, including the parent classes
associated with writing controllers and routers. Common also includes drivers
for generic database access, meaning that the underlying database technology
could be changed fairly trivially if necessary. The current model uses
PyMongo\cite{pymongo} to hook into MongoDB.

The Base program pulls all the these parts together into an application that
runs but needs extending to provide any real functionality. It handles the
loading of configuration files and makes sure they are accessible throughout the
rest of the program. It also spawns the worker threads required to listen on the
message bus. Both a subscriber queue and worker queue are listened to in order
to distinguish between RPC (remote procedure call) messages which always require
a response, and purely informative messages - which require action but no
response. Base also sets up and handles all the logging throughout the program,
ensuring that it gets written to an intuitively named file and in a common
format that makes issues easier to find.
