\section{Introduction}

Data centres - which are collections of computers connected over a local area
network (LAN) - produce vast quantities of data in the form of application and
machine logs. The larger the data centre and the more complex its
applications, the larger the volume of data produced. With the ongoing
industry moving towards “cloud” computing, data centres are becoming ever
larger and being used for a wider variety of applications, often by thousands
or even tens of thousands of users simultaneously. The logs produced by the
services running on the machines which comprise the data centre contain a wide
range of generic (CPU loading, RAM usage, network traffic, etc) and
application-specific data which can be analysed to gain valuable insight into
the operation of the data centre. However, this data is usually unstructured
and unorganized - making meaningful analysis a non-trivial task. When combined
with the sheer volume of data being produced by ever larger data centers, the
gathering and analysis of this data becomes an ever more difficult task.

Solutions already exist to combat this problem (as detailed in the
background section below), but many of these are closed-source and
expensive, and in our opinion those that are open-source do not currently
tackle every aspect of the problem in a complete end-to-end solution. Given
our background in open-source software we were motivated to explore the
development of an open-source project which covered every component
necessary for a user to gain meaningful insight into the workings of their
data centre.
